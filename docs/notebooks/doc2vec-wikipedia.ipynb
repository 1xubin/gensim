{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Doc2Vec on Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the **Document Embedding with Paragraph Vectors** paper, http://arxiv.org/abs/1507.07998.\n",
    "\n",
    "In that paper, the authors only showed results from the DBOW (\"distributed bag of words\") mode, trained on the English Wikipedia. Here we replicate this experiment using not only DBOW, but also the DM mode of the \"paragraph vector\" algorithm aka Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary modules and set up logging. The code below assumes Python 3.7+ and Gensim 4.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "from gensim.corpora.wikicorpus import WikiCorpus, tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [here](http://download.wikimedia.org/enwiki/). You want the file named `enwiki-latest-pages-articles.xml.bz2`.\n",
    "\n",
    "Second, convert that Wikipedia article dump from the arcane Wikimedia XML format into a plain text file. This will make the subsequent training faster and also allow easy inspection of the data = \"input eyeballing\".\n",
    "\n",
    "We'll preprocess each article at the same time, normalizing its text to lowercase, splitting into tokens, etc. Below I use a regexp tokenizer that simply looks for alphabetic sequences as tokens. But feel free to adapt the text preprocessing to your own domain. High quality preprocessing is often critical for the final pipeline accuracy – garbage in, garbage out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus(\n",
    "    \"enwiki-latest-pages-articles.xml.bz2\",  # path to the file you downloaded above\n",
    "    tokenizer_func=tokenize,  # simple regexp; plug in your own tokenizer here\n",
    "    metadata=True,  # also return the article titles and ids when parsing\n",
    "    dictionary={},  # don't start processing the data yet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/work/workspace/gensim/trunk/gensim/utils.py:1332: UserWarning: detected OSX with python3.8+; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n",
      "2022-03-17 21:15:32,118 : INFO : processing article #0: 'Anarchism' (6538 tokens)\n",
      "2022-03-17 21:30:00,138 : INFO : processing article #500000: 'Spiritual Formation Bible' (54 tokens)\n",
      "2022-03-17 21:40:22,219 : INFO : processing article #1000000: 'Adolf von Liebenberg' (52 tokens)\n",
      "2022-03-17 21:49:43,825 : INFO : processing article #1500000: 'Small nucleolar RNA U6-53/MBII-28' (123 tokens)\n",
      "2022-03-17 21:59:23,620 : INFO : processing article #2000000: 'Xie Fei' (50 tokens)\n",
      "2022-03-17 22:09:17,460 : INFO : processing article #2500000: 'Rhein, Saskatchewan' (185 tokens)\n",
      "2022-03-17 22:19:39,293 : INFO : processing article #3000000: 'Kunyinsky District' (969 tokens)\n",
      "2022-03-17 22:30:41,221 : INFO : processing article #3500000: 'Lake Saint-Charles' (555 tokens)\n",
      "2022-03-17 22:41:17,487 : INFO : processing article #4000000: 'Mahāyānasaṃgraha' (612 tokens)\n",
      "2022-03-17 22:52:27,834 : INFO : processing article #4500000: 'Liriomyza trifolii' (1493 tokens)\n",
      "2022-03-17 23:04:41,464 : INFO : processing article #5000000: 'Daniel O. Griffin' (594 tokens)\n",
      "2022-03-17 23:08:58,451 : INFO : finished iterating over Wikipedia corpus of 5176019 documents with 2996051328 positions (total 21837336 articles, 3072543084 positions before pruning articles shorter than 50 words)\n"
     ]
    }
   ],
   "source": [
    "with smart_open.open(\"wiki.txt.gz\", \"w\", encoding='utf8') as fout:\n",
    "    for article_no, (content, (page_id, title)) in enumerate(wiki.get_texts()):\n",
    "        title = ' '.join(title.split())\n",
    "        if article_no % 500000 == 0:\n",
    "            logging.info(\"processing article #%i: %r (%i tokens)\", article_no, title, len(content))\n",
    "        fout.write(f\"{title}\\t{' '.join(content)}\\n\")  # title_of_article [TAB] words of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above takes about 2 hours on my 2021 M1 MacbookPro, and creates a new ~5.8 GB file named `wiki.txt.gz`. We're compressing the text into `.gz` (GZIP) right away to save on disk space, using the [smart_open](https://github.com/RaRe-Technologies/smart_open) library.\n",
    "\n",
    "Next we'll set up a stream to load the preprocessed articles from `wiki.txt.gz` one by one, in the format expected by Doc2Vec, ready for training. We don't want to load everything into RAM at once, because that would blow up the memory. And it is not necessary – Gensim can handle streamed training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiCorpus:\n",
    "    def __init__(self, wiki_text_path):\n",
    "        self.wiki_text_path = wiki_text_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in smart_open.open(self.wiki_text_path, encoding='utf8'):\n",
    "            title, words = line.split('\\t')\n",
    "            yield TaggedDocument(words=words.split(), tags=[title])\n",
    "\n",
    "documents = TaggedWikiCorpus('wiki.txt.gz')  # A streamed iterable; nothing in RAM yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anarchism'] :  anarchism is political philosophy and movement that is sceptical of authority and rejects all involuntary coercive forms of hierarchy anarchism calls for the abolition of the state which it holds to be unnecessary undesirable and harmful as historically left wing movement placed on the farthest left of the political spectrum ……… criticism of philosophical anarchism defence of philosophical anarchism stating that both kinds of anarchism philosophical and political anarchism are philosophical and political claims anarchistic popular fiction novel an argument for philosophical anarchism external links anarchy archives anarchy archives is an online research center on the history and theory of anarchism\n"
     ]
    }
   ],
   "source": [
    "# Load and print the first preprocessed Wikipedia document, as a sanity check = \"input eyeballing\".\n",
    "first_doc = next(iter(documents))\n",
    "print(first_doc.tags, ': ', ' '.join(first_doc.words[:50] + ['………'] + first_doc.words[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document seems legit so let's move on to finally training some Doc2vec models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper had a vocabulary size of 915,715 word types, so we'll try to match it by setting `max_final_vocab=915715` in the Doc2vec constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 23:12:37,360 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t10>', 'datetime': '2022-03-17T23:12:37.360576', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2022-03-17 23:12:37,365 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t10>', 'datetime': '2022-03-17T23:12:37.365118', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, vector_size=200, window=8, epochs=10, workers=cores, max_final_vocab=915715),\n",
    "    # PV-DM with average\n",
    "    Doc2Vec(dm=1, dm_mean=1, vector_size=200, window=8, epochs=10, workers=cores, max_final_vocab=915715),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 23:14:38,521 : INFO : collecting all words and their counts\n",
      "2022-03-17 23:14:38,529 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2022-03-17 23:16:33,505 : INFO : PROGRESS: at example #500000, processed 654950164 words (5696698/s), 3222179 word types, 500000 tags\n",
      "2022-03-17 23:17:41,900 : INFO : PROGRESS: at example #1000000, processed 1018611068 words (5317131/s), 4480366 word types, 1000000 tags\n",
      "2022-03-17 23:18:36,271 : INFO : PROGRESS: at example #1500000, processed 1305140647 words (5269927/s), 5420104 word types, 1500000 tags\n",
      "2022-03-17 23:19:23,908 : INFO : PROGRESS: at example #2000000, processed 1550245240 words (5145361/s), 6188355 word types, 2000000 tags\n",
      "2022-03-17 23:20:10,242 : INFO : PROGRESS: at example #2500000, processed 1790661139 words (5188872/s), 6941128 word types, 2500000 tags\n",
      "2022-03-17 23:20:56,600 : INFO : PROGRESS: at example #3000000, processed 2028261627 words (5125392/s), 7664997 word types, 3000000 tags\n",
      "2022-03-17 23:21:41,918 : INFO : PROGRESS: at example #3500000, processed 2264063867 words (5203393/s), 8347719 word types, 3500000 tags\n",
      "2022-03-17 23:22:25,048 : INFO : PROGRESS: at example #4000000, processed 2488354257 words (5200461/s), 8971529 word types, 4000000 tags\n",
      "2022-03-17 23:23:07,487 : INFO : PROGRESS: at example #4500000, processed 2703313059 words (5065249/s), 9605666 word types, 4500000 tags\n",
      "2022-03-17 23:23:50,776 : INFO : PROGRESS: at example #5000000, processed 2925111571 words (5123692/s), 10217554 word types, 5000000 tags\n",
      "2022-03-17 23:24:19,393 : INFO : collected 10427023 word types and 5176019 unique tags from a corpus of 5176019 examples and 2996051328 words\n",
      "2022-03-17 23:24:22,841 : INFO : Doc2Vec lifecycle event {'msg': 'max_final_vocab=915715 and min_count=5 resulted in calc_min_count=27, effective_min_count=27', 'datetime': '2022-03-17T23:24:22.841740', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-17 23:24:22,842 : INFO : Creating a fresh vocabulary\n",
      "2022-03-17 23:24:26,131 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 retains 894446 unique words (8.578153131531407%% of original 10427023, drops 9532577)', 'datetime': '2022-03-17T23:24:26.131147', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-17 23:24:26,131 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 leaves 2965917340 word corpus (98.99420988824929%% of original 2996051328, drops 30133988)', 'datetime': '2022-03-17T23:24:26.131643', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-17 23:24:28,513 : INFO : deleting the raw counts dictionary of 10427023 items\n",
      "2022-03-17 23:24:28,581 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2022-03-17 23:24:28,581 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2412497836.8123784 word corpus (81.3%% of prior 2965917340)', 'datetime': '2022-03-17T23:24:28.581828', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-03-17 23:24:32,724 : INFO : estimated required memory for 894446 words and 200 dimensions: 7054355600 bytes\n",
      "2022-03-17 23:24:32,725 : INFO : resetting layer weights\n",
      "2022-03-17 23:24:37,804 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t10>\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t10>\n"
     ]
    }
   ],
   "source": [
    "models[0].build_vocab(documents, progress_per=500000)\n",
    "print(models[0])\n",
    "\n",
    "# Save some time by copying the vocabulary structures from the first model.\n",
    "# Both models are built on top of exactly the same data, so there's no need to repeat the vocab-building step.\n",
    "models[1].reset_from(models[0])\n",
    "print(models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to train Doc2Vec on the English Wikipedia. **Warning!** Training the DBOW model takes ~16 hours, and DM ~4 hours, on my 2021 laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 23:29:03,317 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2022-03-17 23:29:03,320 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 10 workers on 894446 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2022-03-17T23:29:03.320153', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-03-17 23:29:04,361 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 379389 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-17 23:59:04,372 : INFO : EPOCH 1 - PROGRESS: at 17.95% examples, 429937 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 00:29:04,379 : INFO : EPOCH 1 - PROGRESS: at 55.55% examples, 437068 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 00:59:04,423 : INFO : EPOCH 1 - PROGRESS: at 98.13% examples, 439343 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 01:00:11,996 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 01:00:12,013 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 01:00:12,028 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 01:00:12,045 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 01:00:12,084 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 01:00:12,110 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 01:00:12,124 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 01:00:12,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 01:00:12,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 01:00:12,149 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 01:00:12,150 : INFO : EPOCH - 1 : training on 2996051328 raw words (2402988821 effective words) took 5468.8s, 439397 effective words/s\n",
      "2022-03-18 01:00:13,169 : INFO : EPOCH 2 - PROGRESS: at 0.00% examples, 390039 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 01:30:13,186 : INFO : EPOCH 2 - PROGRESS: at 19.41% examples, 451763 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 02:00:13,162 : INFO : EPOCH 2 - PROGRESS: at 57.23% examples, 446954 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 02:30:05,143 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 02:30:05,151 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 02:30:05,152 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 02:30:05,162 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 02:30:05,206 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 02:30:05,229 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 02:30:05,232 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 02:30:05,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 02:30:05,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 02:30:05,255 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 02:30:05,255 : INFO : EPOCH - 2 : training on 2996051328 raw words (2402947663 effective words) took 5393.0s, 445566 effective words/s\n",
      "2022-03-18 02:30:06,266 : INFO : EPOCH 3 - PROGRESS: at 0.00% examples, 414962 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 03:00:06,339 : INFO : EPOCH 3 - PROGRESS: at 19.29% examples, 449902 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 03:30:06,314 : INFO : EPOCH 3 - PROGRESS: at 57.27% examples, 447187 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 03:59:56,898 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 03:59:56,905 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 03:59:56,908 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 03:59:56,919 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 03:59:56,982 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 03:59:56,989 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 03:59:57,008 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 03:59:57,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 03:59:57,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 03:59:57,034 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 03:59:57,034 : INFO : EPOCH - 3 : training on 2996051328 raw words (2402969567 effective words) took 5391.8s, 445672 effective words/s\n",
      "2022-03-18 03:59:58,059 : INFO : EPOCH 4 - PROGRESS: at 0.00% examples, 400337 words/s, in_qsize 19, out_qsize 1\n",
      "2022-03-18 04:29:58,091 : INFO : EPOCH 4 - PROGRESS: at 19.41% examples, 451678 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 04:59:58,167 : INFO : EPOCH 4 - PROGRESS: at 57.02% examples, 445731 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 05:29:58,234 : INFO : EPOCH 4 - PROGRESS: at 99.74% examples, 444166 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 05:30:07,257 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 05:30:07,259 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 05:30:07,262 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 05:30:07,296 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 05:30:07,321 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 05:30:07,327 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 05:30:07,337 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 05:30:07,360 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 05:30:07,363 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 05:30:07,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 05:30:07,395 : INFO : EPOCH - 4 : training on 2996051328 raw words (2402983106 effective words) took 5410.2s, 444155 effective words/s\n",
      "2022-03-18 05:30:08,414 : INFO : EPOCH 5 - PROGRESS: at 0.00% examples, 407411 words/s, in_qsize 19, out_qsize 1\n",
      "2022-03-18 06:00:08,451 : INFO : EPOCH 5 - PROGRESS: at 19.32% examples, 450435 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 06:30:08,480 : INFO : EPOCH 5 - PROGRESS: at 57.18% examples, 446664 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 07:00:08,481 : INFO : EPOCH 5 - PROGRESS: at 99.48% examples, 443405 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 07:00:27,046 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 07:00:27,057 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 07:00:27,070 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 07:00:27,083 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 07:00:27,103 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 07:00:27,110 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 07:00:27,117 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 07:00:27,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 07:00:27,151 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 07:00:27,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 07:00:27,156 : INFO : EPOCH - 5 : training on 2996051328 raw words (2402973707 effective words) took 5419.7s, 443375 effective words/s\n",
      "2022-03-18 07:00:28,166 : INFO : EPOCH 6 - PROGRESS: at 0.00% examples, 411123 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 07:30:28,233 : INFO : EPOCH 6 - PROGRESS: at 19.44% examples, 452181 words/s, in_qsize 18, out_qsize 1\n",
      "2022-03-18 08:00:28,218 : INFO : EPOCH 6 - PROGRESS: at 57.42% examples, 447976 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 08:30:28,213 : INFO : EPOCH 6 - PROGRESS: at 99.29% examples, 442848 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 08:30:54,071 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 08:30:54,085 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 08:30:54,094 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 08:30:54,131 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 08:30:54,132 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 08:30:54,145 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 08:30:54,164 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 08:30:54,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 08:30:54,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 08:30:54,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 08:30:54,189 : INFO : EPOCH - 6 : training on 2996051328 raw words (2402970085 effective words) took 5427.0s, 442777 effective words/s\n",
      "2022-03-18 08:30:55,193 : INFO : EPOCH 7 - PROGRESS: at 0.00% examples, 410013 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 09:00:55,267 : INFO : EPOCH 7 - PROGRESS: at 18.94% examples, 444759 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 09:30:55,268 : INFO : EPOCH 7 - PROGRESS: at 55.80% examples, 438741 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 10:00:55,370 : INFO : EPOCH 7 - PROGRESS: at 96.36% examples, 433564 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 10:03:28,340 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 10:03:28,355 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 10:03:28,356 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 10:03:28,376 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 10:03:28,384 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 10:03:28,418 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 10:03:28,419 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 10:03:28,428 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 10:03:28,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 10:03:28,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 10:03:28,468 : INFO : EPOCH - 7 : training on 2996051328 raw words (2402959910 effective words) took 5554.2s, 432641 effective words/s\n",
      "2022-03-18 10:03:29,479 : INFO : EPOCH 8 - PROGRESS: at 0.00% examples, 369271 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 10:33:29,480 : INFO : EPOCH 8 - PROGRESS: at 17.46% examples, 422524 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 11:03:29,445 : INFO : EPOCH 8 - PROGRESS: at 50.17% examples, 408225 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 11:33:29,516 : INFO : EPOCH 8 - PROGRESS: at 90.78% examples, 414606 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 11:39:44,147 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 11:39:44,166 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 11:39:44,171 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 11:39:44,186 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 11:39:44,215 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 11:39:44,226 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 11:39:44,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 11:39:44,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 11:39:44,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 11:39:44,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 11:39:44,268 : INFO : EPOCH - 8 : training on 2996051328 raw words (2402978662 effective words) took 5775.8s, 416044 effective words/s\n",
      "2022-03-18 11:39:45,288 : INFO : EPOCH 9 - PROGRESS: at 0.00% examples, 394893 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 12:09:45,255 : INFO : EPOCH 9 - PROGRESS: at 19.03% examples, 446055 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 12:39:45,321 : INFO : EPOCH 9 - PROGRESS: at 56.60% examples, 443313 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 13:09:45,381 : INFO : EPOCH 9 - PROGRESS: at 97.33% examples, 436797 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 13:11:27,488 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 13:11:27,492 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 13:11:27,504 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 13:11:27,535 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 13:11:27,552 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 13:11:27,553 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 13:11:27,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 13:11:27,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 13:11:27,584 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 13:11:27,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 13:11:27,592 : INFO : EPOCH - 9 : training on 2996051328 raw words (2402988333 effective words) took 5503.3s, 436648 effective words/s\n",
      "2022-03-18 13:11:28,615 : INFO : EPOCH 10 - PROGRESS: at 0.00% examples, 391894 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 13:41:28,611 : INFO : EPOCH 10 - PROGRESS: at 17.83% examples, 428194 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 14:11:28,622 : INFO : EPOCH 10 - PROGRESS: at 51.72% examples, 416555 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 14:41:28,621 : INFO : EPOCH 10 - PROGRESS: at 91.60% examples, 417311 words/s, in_qsize 19, out_qsize 0\n",
      "2022-03-18 14:47:23,420 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 14:47:23,432 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 14:47:23,433 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 14:47:23,437 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 14:47:23,469 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 14:47:23,484 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 14:47:23,502 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 14:47:23,512 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 14:47:23,516 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 14:47:23,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 14:47:23,534 : INFO : EPOCH - 10 : training on 2996051328 raw words (2402969667 effective words) took 5755.9s, 417476 effective words/s\n",
      "2022-03-18 14:47:23,536 : INFO : Doc2Vec lifecycle event {'msg': 'training on 29960513280 raw words (24029729521 effective words) took 55099.9s, 436112 effective words/s', 'datetime': '2022-03-18T14:47:23.536569', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-03-18 14:47:23,537 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2022-03-18 14:47:23,537 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 10 workers on 894446 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2022-03-18T14:47:23.537351', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-03-18 14:47:24,546 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 121520 words/s, in_qsize 20, out_qsize 0\n",
      "2022-03-18 15:12:45,307 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 15:12:45,325 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 15:12:45,326 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 15:12:45,327 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 15:12:45,327 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 15:12:45,332 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 15:12:45,338 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 15:12:45,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 15:12:45,348 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 15:12:45,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 15:12:45,355 : INFO : EPOCH - 1 : training on 2996051328 raw words (2402951760 effective words) took 1521.7s, 1579074 effective words/s\n",
      "2022-03-18 15:12:46,373 : INFO : EPOCH 2 - PROGRESS: at 0.01% examples, 1835607 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 15:38:18,272 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 15:38:18,310 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 15:38:18,312 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 15:38:18,313 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 15:38:18,319 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 15:38:18,320 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 15:38:18,325 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 15:38:18,336 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 15:38:18,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 15:38:18,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 15:38:18,339 : INFO : EPOCH - 2 : training on 2996051328 raw words (2402972271 effective words) took 1533.0s, 1567541 effective words/s\n",
      "2022-03-18 15:38:19,355 : INFO : EPOCH 3 - PROGRESS: at 0.01% examples, 1940890 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 16:02:47,736 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 16:02:47,759 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 16:02:47,762 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 16:02:47,764 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 16:02:47,764 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 16:02:47,775 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 16:02:47,781 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 16:02:47,788 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 16:02:47,789 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 16:02:47,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 16:02:47,791 : INFO : EPOCH - 3 : training on 2996051328 raw words (2402988495 effective words) took 1469.4s, 1635360 effective words/s\n",
      "2022-03-18 16:02:48,814 : INFO : EPOCH 4 - PROGRESS: at 0.01% examples, 2013560 words/s, in_qsize 0, out_qsize 2\n",
      "2022-03-18 16:26:11,222 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 16:26:11,234 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 16:26:11,236 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 16:26:11,239 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 16:26:11,242 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 16:26:11,245 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 16:26:11,254 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 16:26:11,258 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 16:26:11,261 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 16:26:11,262 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 16:26:11,262 : INFO : EPOCH - 4 : training on 2996051328 raw words (2402958098 effective words) took 1403.4s, 1712179 effective words/s\n",
      "2022-03-18 16:26:12,270 : INFO : EPOCH 5 - PROGRESS: at 0.01% examples, 2003817 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 16:50:15,159 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 16:50:15,175 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 16:50:15,176 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 16:50:15,177 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 16:50:15,183 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 16:50:15,186 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 16:50:15,198 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 16:50:15,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 16:50:15,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 16:50:15,207 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 16:50:15,207 : INFO : EPOCH - 5 : training on 2996051328 raw words (2402956752 effective words) took 1443.9s, 1664163 effective words/s\n",
      "2022-03-18 16:50:16,219 : INFO : EPOCH 6 - PROGRESS: at 0.01% examples, 1987406 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 17:13:45,624 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 17:13:45,632 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 17:13:45,635 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 17:13:45,636 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 17:13:45,637 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 17:13:45,640 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 17:13:45,651 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 17:13:45,657 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 17:13:45,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 17:13:45,665 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 17:13:45,665 : INFO : EPOCH - 6 : training on 2996051328 raw words (2402960350 effective words) took 1410.5s, 1703664 effective words/s\n",
      "2022-03-18 17:13:46,675 : INFO : EPOCH 7 - PROGRESS: at 0.01% examples, 1985995 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 17:37:06,489 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 17:37:06,523 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 17:37:06,524 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 17:37:06,524 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 17:37:06,525 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 17:37:06,529 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 17:37:06,537 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 17:37:06,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 17:37:06,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 17:37:06,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 17:37:06,546 : INFO : EPOCH - 7 : training on 2996051328 raw words (2402972246 effective words) took 1400.9s, 1715338 effective words/s\n",
      "2022-03-18 17:37:07,560 : INFO : EPOCH 8 - PROGRESS: at 0.01% examples, 2069561 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 18:00:31,024 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 18:00:31,053 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 18:00:31,056 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 18:00:31,057 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 18:00:31,059 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 18:00:31,059 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 18:00:31,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 18:00:31,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 18:00:31,075 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 18:00:31,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 18:00:31,076 : INFO : EPOCH - 8 : training on 2996051328 raw words (2402970402 effective words) took 1404.5s, 1710899 effective words/s\n",
      "2022-03-18 18:00:32,091 : INFO : EPOCH 9 - PROGRESS: at 0.01% examples, 2063533 words/s, in_qsize 0, out_qsize 0\n",
      "2022-03-18 18:23:47,471 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 18:23:47,482 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 18:23:47,485 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 18:23:47,489 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 18:23:47,490 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 18:23:47,492 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 18:23:47,497 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 18:23:47,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 18:23:47,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 18:23:47,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 18:23:47,510 : INFO : EPOCH - 9 : training on 2996051328 raw words (2402978209 effective words) took 1396.4s, 1720847 effective words/s\n",
      "2022-03-18 18:23:48,527 : INFO : EPOCH 10 - PROGRESS: at 0.01% examples, 1857646 words/s, in_qsize 11, out_qsize 0\n",
      "2022-03-18 18:47:53,356 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2022-03-18 18:47:53,364 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2022-03-18 18:47:53,368 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2022-03-18 18:47:53,370 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2022-03-18 18:47:53,370 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2022-03-18 18:47:53,372 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2022-03-18 18:47:53,377 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-03-18 18:47:53,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-18 18:47:53,385 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-18 18:47:53,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-18 18:47:53,390 : INFO : EPOCH - 10 : training on 2996051328 raw words (2402975872 effective words) took 1445.9s, 1661955 effective words/s\n",
      "2022-03-18 18:47:53,391 : INFO : Doc2Vec lifecycle event {'msg': 'training on 29960513280 raw words (24029684455 effective words) took 14429.7s, 1665293 effective words/s', 'datetime': '2022-03-18T18:47:53.391169', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs, report_delay=30*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's test both models! The DBOW model shows similar results as the original paper.\n",
    "\n",
    "First, calculate the most similar Wikipedia articles to the \"Machine learning\" article. The calculated word vectors and document vectors are stored separately, in `model.wv` and `model.dv` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t10>\n",
      "[('Supervised learning', 0.7626678943634033),\n",
      " ('Pattern recognition', 0.7443839907646179),\n",
      " ('Artificial neural network', 0.7443667650222778),\n",
      " ('Boosting (machine learning)', 0.7209591865539551),\n",
      " ('Deep learning', 0.7030681371688843),\n",
      " ('Linear classifier', 0.6918482184410095),\n",
      " ('Feature selection', 0.6885010600090027),\n",
      " ('Knowledge retrieval', 0.6797034740447998),\n",
      " ('Convolutional neural network', 0.6789148449897766),\n",
      " ('Outline of computer science', 0.6732515096664429),\n",
      " ('Training, validation, and test sets', 0.6729527711868286),\n",
      " ('Support-vector machine', 0.6719434857368469),\n",
      " ('Learning classifier system', 0.6716565489768982),\n",
      " ('Outline of machine learning', 0.6692107915878296),\n",
      " ('Bayesian network', 0.6654112935066223),\n",
      " ('Manifold regularization', 0.6635575294494629),\n",
      " ('Multi-task learning', 0.6624512672424316),\n",
      " ('Fuzzy logic', 0.6605969667434692),\n",
      " ('Computer mathematics', 0.6600310206413269),\n",
      " ('Recurrent neural network', 0.6571199893951416)]\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t10>\n",
      "[('Pattern recognition', 0.731984555721283),\n",
      " ('Supervised learning', 0.7107947468757629),\n",
      " ('Multi-task learning', 0.6985798478126526),\n",
      " ('Semi-supervised learning', 0.6792073249816895),\n",
      " ('Meta learning (computer science)', 0.6784282922744751),\n",
      " ('Support-vector machine', 0.6740356683731079),\n",
      " ('Feature selection', 0.6702772378921509),\n",
      " ('Statistical learning theory', 0.6683863997459412),\n",
      " ('Automatic image annotation', 0.661750078201294),\n",
      " ('Deep learning', 0.6617218255996704),\n",
      " ('Linear classifier', 0.6573296189308167),\n",
      " ('Statistical classification', 0.654957115650177),\n",
      " ('Regularization (mathematics)', 0.6517974138259888),\n",
      " ('Data analysis techniques for fraud detection', 0.6505621671676636),\n",
      " ('Artificial neural network', 0.6478281021118164),\n",
      " ('Boosting (machine learning)', 0.6463974714279175),\n",
      " ('Naive Bayes classifier', 0.6442222595214844),\n",
      " ('Autoencoder', 0.6438822746276855),\n",
      " ('Predictive Model Markup Language', 0.6405109763145447),\n",
      " ('Perceptron', 0.6379765868186951)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both results seem similar, but note the DM model took 4x less time train (training 4x faster).\n",
    "\n",
    "Second, let's calculate the most similar Wikipedia entries to \"Lady Gaga\" using Paragraph Vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t10>\n",
      "[('Ariana Grande', 0.755739688873291),\n",
      " ('Katy Perry', 0.7534462213516235),\n",
      " ('Miley Cyrus', 0.7091007828712463),\n",
      " ('Adele', 0.6958011984825134),\n",
      " ('Demi Lovato', 0.6867919564247131),\n",
      " ('Nicki Minaj', 0.6783465147018433),\n",
      " ('Taylor Swift', 0.6691418886184692),\n",
      " ('Adam Lambert', 0.6638894081115723),\n",
      " ('Rihanna', 0.6437391638755798),\n",
      " ('Kesha', 0.6433634161949158)]\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t10>\n",
      "[('Born This Way (album)', 0.6649508476257324),\n",
      " ('Artpop', 0.6616811752319336),\n",
      " ('Lady Gaga videography', 0.6363328695297241),\n",
      " ('Katy Perry', 0.6322777271270752),\n",
      " ('Beautiful, Dirty, Rich', 0.6277879476547241),\n",
      " ('Lady Gaga discography', 0.60688316822052),\n",
      " ('Applause (Lady Gaga song)', 0.6062529683113098),\n",
      " ('List of Lady Gaga live performances', 0.5975069403648376),\n",
      " ('Born This Way (song)', 0.5948888659477234),\n",
      " ('Madonna', 0.5918263792991638)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Lady Gaga\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The DBOW model reveals similar singers in the U.S., while the DM model seems to pay more attention to the word \"Gaga\" itself.\n",
    "\n",
    "Finally, let's do some wilder artihmetics that vectors embeddings are famous for. What are the entries most similar to \"Lady Gaga\" - \"American\" + \"Japanese\"?\n",
    "\n",
    "Note that \"American\" and \"Japanese\" are word vectors, but they live in the same space as the document vectors so we can add / subtract them at will, for some interesting results. All word vectors were already lowercased by our tokenizer above, so we look for the lowercased version here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t10>\n",
      "[('Ayumi Hamasaki', 0.604461669921875),\n",
      " ('2NE1', 0.5942890644073486),\n",
      " ('Katy Perry', 0.5932046175003052),\n",
      " ('Ariana Grande', 0.5865142941474915),\n",
      " (\"Can't Stop the Disco\", 0.5778986215591431),\n",
      " (\"Girls' Generation\", 0.5741134285926819),\n",
      " ('We Are \"Lonely Girl\"', 0.5682086944580078),\n",
      " ('Perfume (Japanese band)', 0.568188488483429),\n",
      " ('H (Ayumi Hamasaki EP)', 0.5679325461387634),\n",
      " ('Kyary Pamyu Pamyu', 0.5665541887283325)]\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t10>\n",
      "[('Kaela Kimura', 0.5528751015663147),\n",
      " ('Chisato Moritaka', 0.551906943321228),\n",
      " ('Suzuki Ami Around the World: Live House Tour 2005', 0.5428911447525024),\n",
      " ('Pink Lady (duo)', 0.5385505557060242),\n",
      " ('Artpop', 0.5361125469207764),\n",
      " ('Kaede (dancer)', 0.535369873046875),\n",
      " ('Miliyah Kato', 0.5336685180664062),\n",
      " ('Liyuu', 0.5325193405151367),\n",
      " ('Ai (singer)', 0.5272262692451477),\n",
      " ('Momoiro Clover Z', 0.525260329246521)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    vec = [model.dv[\"Lady Gaga\"] - model.wv[\"american\"] + model.wv[\"japanese\"]]\n",
    "    pprint([m for m in model.dv.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the DBOW model surfaced artists similar to Lady Gaga in Japan, such as **Ayumi Hamasaki** whose Wiki bio says:\n",
    "\n",
    "> Ayumi Hamasaki is a Japanese singer, songwriter, record producer, actress, model, spokesperson, and entrepreneur.\n",
    "\n",
    "So that sounds like a success.\n",
    "\n",
    "Similarly, the DM model thought **Kaela Kimura** is the closest hit:\n",
    "\n",
    "> Kaela Kimura is a Japanese pop rock singer, lyricist, fashion model and television presenter.\n",
    "\n",
    "Also pretty good.\n",
    "\n",
    "These results demonstrate that both training modes employed in the original paper are outstanding for calculating similarity between document vectors, word vectors, or a combination of both. The DM mode has the added advantage of being 4x faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to continue working with these trained models, you could save them to disk, to avoid having to re-train the models from scratch every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 19:08:34,623 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_dbow.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-03-18T19:08:34.622990', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'saving'}\n",
      "2022-03-18 19:08:34,641 : INFO : storing np array 'vectors' to doc2vec_dbow.model.dv.vectors.npy\n",
      "2022-03-18 19:08:40,244 : INFO : storing np array 'vectors' to doc2vec_dbow.model.wv.vectors.npy\n",
      "2022-03-18 19:08:46,811 : INFO : storing np array 'syn1neg' to doc2vec_dbow.model.syn1neg.npy\n",
      "2022-03-18 19:08:48,564 : INFO : not storing attribute cum_table\n",
      "2022-03-18 19:08:56,097 : INFO : saved doc2vec_dbow.model\n",
      "2022-03-18 19:08:56,098 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_dm.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-03-18T19:08:56.098765', 'gensim': '4.1.3.dev0', 'python': '3.8.9 (default, Oct 26 2021, 07:25:53) \\n[Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-12.2.1-arm64-arm-64bit', 'event': 'saving'}\n",
      "2022-03-18 19:08:56,099 : INFO : storing np array 'vectors' to doc2vec_dm.model.dv.vectors.npy\n",
      "2022-03-18 19:09:09,087 : INFO : storing np array 'vectors' to doc2vec_dm.model.wv.vectors.npy\n",
      "2022-03-18 19:09:13,804 : INFO : storing np array 'syn1neg' to doc2vec_dm.model.syn1neg.npy\n",
      "2022-03-18 19:09:16,101 : INFO : not storing attribute cum_table\n",
      "2022-03-18 19:09:20,432 : INFO : saved doc2vec_dm.model\n"
     ]
    }
   ],
   "source": [
    "models[0].save('doc2vec_dbow.model')\n",
    "models[1].save('doc2vec_dm.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue your doc2vec explorations, refer to the official API documentation in Gensim: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
